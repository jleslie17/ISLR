{
    "collab_server" : "",
    "contents" : "# 5.3 Lab: Cross-validation and the Bootstrap\n\n# 5.3.1 The validation set approach\nlibrary(ISLR)\nset.seed(1)\ntrain <- sample(392, 196)\n?sample\nlm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)\n\nattach(Auto)\nmean((mpg - predict(lm.fit, Auto))[-train] ^ 2)\n# Therefore the estimated test MSE for the linear regressino fit is 26.14. We\n# can use the poly() function to estimate the test error for the polynomial and\n# cubic regressions\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)\nmean((mpg - predict(lm.fit2, Auto))[-train]^2)\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)\nmean((mpg - predict(lm.fit3, Auto))[-train]^2)\n\nset.seed(2)\ntrain <- sample(392, 196)\nlm.fit <- lm(mpg ~ horsepower, subset = train)\nmean((mpg - predict(lm.fit, Auto))[-train]^2)\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), subset = train)\nmean((mpg - predict(lm.fit2, Auto))[-train] ^ 2)\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), subset = train)\nmean((mpg - predict(lm.fit3, Auto))[-train] ^ 2)\n\n# 5.3.2 Leave-one-out cross-validation\nlibrary(boot)\nglm.fit <- glm(mpg ~ horsepower, data = Auto)\ncv.err <- cv.glm(Auto, glm.fit)\ncv.err$delta\n?cv.glm\n\ncv.error <- rep(0, 5)\nfor (i in 1:5) {\n  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]\n}\ncv.error\n\n# 5.3.3 k-fold cros-validation\nset.seed(17)\ncv.error.10 <- rep(0, 10)\nfor (i in 1:10) {\n  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]\n}\ncv.error.10\n\n# 5.3.4 The Bootstrap\nalpha.fn <- function(data, index) {\n  X <- data$X[index]\n  Y <- data$Y[index]\n  return((var(Y) - cov(X, Y))/(var(X) + var(Y) - 2*cov(X,Y)))\n}\nalpha.fn(Portfolio, 1:100)\n\nset.seed(1)\nalpha.fn(Portfolio, sample(100, 100, replace = T))\n\nboot(Portfolio, alpha.fn, R = 1000)\n\nboot.fn <- function(data, index) \n  return(coef(lm(mpg ~ horsepower, data = data, subset = index)))\nboot.fn(Auto, 1:392)\nset.seed(1)\nboot.fn(Auto, sample(392, 392, replace = T))\nboot(Auto, boot.fn, 1000)\nsummary(lm(mpg ~ horsepower, data = Auto))$coef\n# Bootstrap gives more accurate estimates of standard errors of B0 and B1 than\n# the summary() function\n\n# Try quadratic for the same...should be a better fit\nboot.fn <- function(data, index)\n  coefficients(lm(mpg ~ horsepower + I(horsepower^2), \n                  data = data, subset = index))\nset.seed(1)\nboot(Auto, boot.fn, 1000)\nsummary(lm(mpg ~ horsepower + I(horsepower ^ 2), \n           data = Auto))$coef\ndetach(Auto)\n\n# 5.4 Exercises\n# 5\nattach(Default)\nset.seed(1)\n# a\nglm.fit <- glm(\n  default ~ income + balance,\n  data = Default,\n  family = binomial\n)\nsummary(glm.fit)\n\n# b\ntrain <- sample(dim(Default)[1], dim(Default)[1]/2)\nglm.fit <- glm(\n  default ~ income + balance,\n  data = Default,\n  family = binomial,\n  subset = train\n)\n# summary(glm.fit)\nglm.probs <- predict(\n  glm.fit,\n  newdata = Default[-train,],\n  type = \"response\"\n)\nglm.pred <- rep(\"No\", length(train))\nglm.pred[glm.probs > 0.5] <- \"Yes\"\nmean(glm.pred != Default$default[-train])\n\n# d\nhead(Default)\nset.seed(1)\ntrain <- sample(dim(Default)[1], dim(Default)[1]/2)\nglm.pred <- glm(\n  default ~ income + balance + student,\n  data = Default,\n  family = binomial,\n  subset = train\n)\nglm.probs <- predict(glm.fit, Default[-train, ], type = \"response\")\nglm.pred <- rep(\"No\", length(train))\nglm.pred[glm.probs > 0.5] <- \"Yes\"\nmean(glm.pred != Default$default[-train])\n# Adding dummy variables doesn't appear to have an effect\n\n# 6\n# a\nset.seed(1)\nglm.fit <- glm(\n  default ~ income + balance,\n  data = Default,\n  family = binomial\n)\nsummary(glm.fit)\n# glm() esimates for SE of B0, B1 and B2 are 0.43, 4.99^-6 and 2.27^-4\n# b\nboot.fn <- function(data, index) {\n  fit <- glm(default ~ income + balance, \n             data = data,\n             family = binomial,\n             subset = index)\n  return(coef(fit))\n}\nboot.fn(Default, 1:1000)\n# c\nlibrary(boot)\nboot(Default, boot.fn, 1000)\n# d\n# Not much difference in the estimated std errors\ndetach(Auto)\n\n# 7\n# a\nattach(Weekly)\nhead(Weekly)\nset.seed(1)\nglm.fit <- glm(\n  Direction ~ Lag1 + Lag2,\n  data = Weekly,\n  family = binomial\n)\n# b\nglm.fit.not1 <- glm(\n  Direction ~ Lag1 + Lag2,\n  data = Weekly,\n  family = binomial,\n  subset = -1\n)\nsummary(glm.fit)\nsummary(glm.fit.not1)\n# c\nfirst_obs <- predict(\n  glm.fit.not1,\n  newdata = Weekly[1, ],\n  type = \"response\"\n)\nfirst_obs > 0.5\nDirection[1]\n# Incorrectly classified\n# d\nerror <- rep(0, nrow(Weekly))\nfor (i in 1:nrow(Weekly)) {\n  glm.fit <- glm(\n    Direction ~ Lag1 + Lag2,\n    data = Weekly,\n    family = binomial,\n    subset = -i\n  )\n  glm.probs <- predict(\n    glm.fit,\n    Weekly[i,],\n    type = \"response\"\n  )\n  if (glm.probs > 0.5) {\n    glm.pred <- \"Up\"\n  } else {\n    glm.pred <- \"Down\"\n  }\n  error[i] <- glm.pred != Weekly$Direction[i]\n}\n# e\nmean(error)\n# The LOOCV estimate for test error rate is 44.995%\n\n# 8\n# a\nset.seed(1)\ny <- rnorm(100)\nx <- rnorm(100)\ny <- x - 2*x^2 + rnorm(100)\n# n = 100, p = 2; y = x - 2x^2 + e\n# b\nplot(x, y)\n# There appears to be a curved relationship\n# c\nlibrary(boot)\nset.seed(1)\nglm.fit <- glm(\n  y ~ x\n)\ncv.error <- cv.glm(\n  data.frame(y = y, x = x),\n  glmfit = glm.fit\n)$delta[1]\ncv.error\nglm.fit2 <- glm(y ~ poly(x, 2))\ncv.glm(data.frame(x = x, y = y), glm.fit2)$delta[1]\n\nglm.fit3 <- glm(y ~ poly(x, 3))\ncv.glm(data.frame(x = x, y = y), glm.fit3)$delta[1]\n\nglm.fit4 <- glm(y ~ poly(x, 4))\ncv.glm(data.frame(x = x, y = y), glm.fit4)$delta[1]\n# e\nset.seed(10)\nglm.fit <- glm(\n  y ~ x\n)\ncv.error <- cv.glm(\n  data.frame(y = y, x = x),\n  glmfit = glm.fit\n)$delta[1]\ncv.error\nglm.fit2 <- glm(y ~ poly(x, 2))\ncv.glm(data.frame(x = x, y = y), glm.fit2)$delta[1]\n\nglm.fit3 <- glm(y ~ poly(x, 3))\ncv.glm(data.frame(x = x, y = y), glm.fit3)$delta[1]\n\nglm.fit4 <- glm(y ~ poly(x, 4))\ncv.glm(data.frame(x = x, y = y), glm.fit4)$delta[1]\n# errors are the same since LOOCV evaluates n folds of a single observation\n# f\nsummary(glm.fit4)\n# The p-values show that the linear and quadratic terms are statistically\n# significants and that the cubic and 4th degree terms are not statistically\n# significants. This agree strongly with our cross-validation results which were\n# minimum for the quadratic model.\ndetach(Weekly)\n\n# 9\nlibrary(MASS)\nattach(Boston)\n# a\nmu_hat <- mean(medv)\nmu_hat\n# b\nse_hat <- sd(medv)/sqrt(nrow(Boston))\nse_hat\n# c\nset.seed(1)\nboot.fn <- function(data, index) {\n  mu <- mean(data[index])\n  return(mu)\n}\nboot.fn(medv, nrow(Boston))\nboot(medv, boot.fn, 1000)\n# d\nt.test(medv)\nCI.mu.hat <- 22.53 + c(-1, 1) * 2 *0.4119\nCI.mu.hat\n# e\nmed.hat <- median(medv)\nmed.hat\n# f\nboot.fn <- function(data, index) {\n  med.hat <- median(data[index])\n  return(med.hat)\n}\nboot(medv, boot.fn, 1000)\n# g\nquantile(medv, probs = 0.1)\n# h\nboot.fn <- function(data, index) {\n  mu.10 <- quantile(medv[index], probs = 0.1)\n  return(mu.10)\n}\nboot(medv, boot.fn, 1000)\n# We get an estimated tenth percentile value of 12.75 which is again equal to\n# the value obtained in (g), with a standard error of 0.5113 which is relatively\n# small compared to percentile value.",
    "created" : 1485354997755.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2798900131",
    "id" : "9F2B5884",
    "lastKnownWriteTime" : 1485377672,
    "last_content_update" : 1485377672199,
    "path" : "~/DataScience/ISLR/5_Resampling_Methods.R",
    "project_path" : "5_Resampling_Methods.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}