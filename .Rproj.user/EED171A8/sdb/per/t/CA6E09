{
    "collab_server" : "",
    "contents" : "library(ISLR)\ndata(Default)\nhead(Default)\nboxplot(Default$default, Default$balance)\nprop.table(table(Default$default))\n\nfit <- glm(default ~ balance, \n           family = \"binomial\",\n           data = Default)\nsummary(fit)\nfit2 <- glm(default ~ student,\n           family = \"binomial\",\n           data = Default)\nsummary(fit2)\npredict(fit2, newdata = data.frame(student <- \"Yes\"))\nexp(predict(fit2, newdata = data.frame(student <- \"Yes\")))\nexp(predict(fit2, newdata = data.frame(student <- \"No\")))\n\nexp(predict(fit, newdata = data.frame(balance = 1000)))\n?glm\n\n# 4.6 Lab ========\nnames(Smarket)\ndata(\"Smarket\")\nhead(Smarket)\ndim(Smarket)\nsummary(Smarket)\npairs(Smarket)\n#cor(Smarket)\ncor(Smarket[-9])\nattach(Smarket)\nplot(Volume)\n?plot\n\n# 4.6.2 Logistic Regression ====\nglm.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n              data = Smarket,\n              family = binomial)\nsummary(glm.fit)\ncoef(glm.fit)\nsummary(glm.fit)$coef\nsummary(glm.fit)$coef[,4]\nglm.probs <- predict(glm.fit, type = \"response\")\nglm.probs[1:10]\ncontrasts(Direction)\nglm.pred <- rep(\"Down\", 1250)\nglm.pred[glm.probs > 0.5] <- \"Up\"\ntable(glm.pred, Direction)\n(507 + 145)/1250\nmean(glm.pred == Direction)\n\ntrain <- (Year < 2005)\nSmarket.2005 <- Smarket[!train, ]\ndim(Smarket.2005)\nDirection.2005 <- Direction[!train]\nglm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n               data = Smarket, family = binomial, subset = train)\nglm.probs <- predict(glm.fit, Smarket.2005, type = \"response\")\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > 0.5] <- \"Up\"\ntable(glm.pred, Direction.2005)\nmean(glm.pred == Direction.2005)\nmean(glm.pred != Direction.2005)\n\nglm.fit <- glm(Direction ~ Lag1 + Lag2,\n               data = Smarket,\n               family = binomial,\n               subset = train)\nsummary(glm.fit)\nglm.probs <- predict(glm.fit, Smarket.2005, type = \"response\")\nglm.probs[1:10]\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > 0.5] <- \"Up\"\ntable(glm.pred, Direction.2005)\nmean(glm.pred == Direction.2005)\nmean(glm.pred != Direction.2005)\n106/(35 + 106)\n141/(141 + 111)\n106/(76 + 106)\ntable(Direction.2005)\n\npredict(glm.fit, \n        newdata = data.frame(Lag1 = c(1.2, 1.5),\n                             Lag2 = c(1.1, -0.8)),\n        type = \"response\")\n\n# 4.6.3 Linear Discriminant Analysis ====\nlibrary(MASS)\nlda.fit <- lda(Direction ~ Lag1 + Lag2,\n               data = Smarket,\n               subset = train)\nlda.fit\nplot(lda.fit)\nlda.pred <- predict(lda.fit, Smarket.2005)\nnames(lda.pred)\nlda.pred[1]\nlda.pred[2]\nlda.pred[3]\n\nlda.class <- lda.pred$class\ntable(lda.class, Direction.2005)\nmean(lda.class == Direction.2005)\nhead(lda.pred[2])\nsum(lda.pred$posterior[,1] >= 0.5)\nsum(lda.pred$posterior[,1] <= 0.5)\nlda.pred$posterior[1:10]\nhead(lda.pred$posterior)\nlda.pred$posterior[1:20, 1]\nlda.class[1:20]\nsum(lda.pred$posterior[,1] > 0.9)\n\n# 4.6.4 Quadratic Discriminant Analysis ====\nqda.fit <- qda(Direction ~ Lag1 + Lag2,\n               data = Smarket,\n               subset = train)\nqda.fit\nqda.class = predict(qda.fit, Smarket.2005)$class\ntable(qda.class, Direction.2005)\nmean(qda.class == Direction.2005)\n\n# 4.6.5 K-Nearest Neighbors\nlibrary(class)\ntrain.X <- cbind(Lag1, Lag2)[train, ]\ntest.X <- cbind(Lag1, Lag2)[!train, ]\ntrain.Direction <- Direction[train]\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Direction, k = 1)\ntable(knn.pred, Direction.2005)\n(83 + 43)/252\nknn.pred <- knn(train.X, test.X, train.Direction, k = 3)\ntable(knn.pred, Direction.2005)\nmean(knn.pred == Direction.2005)\n\n# 4.6.6 An Application to Caravan Insurance Data ====\ndim(Caravan)\ndetach(Smarket)\nattach(Caravan)\nsummary(Purchase)\n348/5822\nstandarized.X <- scale(Caravan[, -86])\nvar(Caravan[,1])\nvar(Caravan[,2])\nvar(standarized.X[,1])\nvar(standarized.X[,2])\n\ntest <- 1:1000\ntrain.X <- standarized.X[-test,]\ntest.X <- standarized.X[test,]\ntrain.Y <- Purchase[-test]\ntest.Y <- Purchase[test]\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Y, k = 1)\nmean(test.Y != knn.pred)\nmean(test.Y != \"No\")\ntable(knn.pred, test.Y)\n9/(68+9)\n\nknn.pred <- knn(train.X, test.X, train.Y, k = 3)\ntable(knn.pred, test.Y)\n5/26\nknn.pred <- knn(train.X, test.X, train.Y, k = 5)\ntable(knn.pred, test.Y)\n4/15\n\nglm.fit <- glm(Purchase ~ .,\n               data = Caravan,\n               family = binomial,\n               subset = -test)\nglm.probs <- predict(glm.fit, Caravan[test, ], type = \"response\")\nglm.pred <- rep(\"No\", 1000)\nglm.pred[glm.probs > 0.5] <- \"Yes\"\ntable(glm.pred, test.Y)\nglm.pred <- rep(\"No\", 1000)\nglm.pred[glm.probs > 0.25] <- \"Yes\"\ntable(glm.pred, test.Y)\n11/(11+22)\n\n# 4.7 Exercises =====\n# 6\nB0 <- -6\nB1 <- 0.05\nB2 <- 1\nx1 <- 40\nx2 <- 3.5\n\n\n\n\np <- exp(B0 + B1*x1 + B2*x2)/(1 + exp(B0 + B1*x1 + B2*x2))\np\np <- 0.5\nx <- (log(p/(1 - p)) - B0 - B2*x2)/B1\nx\n\n# 9\n37/137\n\n0.16/(1 - 0.16)\n\n# 10 ====\ndetach(Caravan)\nrm(list = ls())\n\n# a\ndata(\"Weekly\")\nhead(Weekly)\nnrow(Weekly[Weekly$Year == 1991,])\nattach(Weekly)\nplot(Today)\nplot(Volume)\nsummary(Weekly)\ncor(Weekly[,-9])\n\n# b\n?glm\nfit.glm <- glm(\n  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n  data = Weekly,\n  family = binomial\n)\nsummary(fit.glm)\n# Lag2 appears to be statistically significant\n\n# c\nglm.probs <- predict(fit.glm, type = \"response\")\nglm.pred <- rep(\"Down\", 1089)\nglm.pred[glm.probs > 0.5] <- \"Up\"\ntable(glm.pred, Direction)\nmean(glm.pred == Direction) #Accuracy\n557/(557 + 48) #Sensitivity\n54/(54 + 430) #Specificiy\n430/(430 + 54) #False positive rate\n\n# d\ntrain <- Year < 2009\nglm.fit <- glm(\n  Direction ~ Lag2,\n  data = Weekly,\n  family = binomial,\n  subset = train\n)\nWeekly_test <- Weekly[!train,]\nDirection_test <- Direction[!train]\nglm.probs <- predict(\n  glm.fit, \n  newdata = Weekly_test,\n  type = \"response\")\nglm.pred <- rep(\"Down\", 104)\nglm.pred[glm.probs > 0.5] <- \"Up\"\ntable(glm.pred, Direction_test)\nmean(glm.pred == Direction_test)\n56/(56+5) #True positive rate/Sensitivity\n9/(9+34) #Specificity\n34/(9+34) #False positive rate\n\n# e\nlda.fit <- lda(\n  Direction ~ Lag2,\n  data = Weekly,\n  subset = train\n)\nlda.pred <- predict(\n  lda.fit,\n  newdata = Weekly_test\n)\nnames(lda.pred)\nlda.class <- lda.pred$class\ntable(lda.class, Direction_test)\nmean(lda.class == Direction_test)\n56/(56+5) #True positive rate/Sensitivity\n9/(9+34) #Specificity\n34/(9+34) #False positive rate\n\n# f\nqda.fit <- qda(\n  Direction ~ Lag2,\n  data = Weekly,\n  subset = train\n)\nqda.pred <- predict(\n  qda.fit,\n  Weekly_test\n)\nnames(qda.pred)\nqda.class <- qda.pred$class\ntable(qda.class, Direction_test)\nmean(qda.class == Direction_test)\n\n# g\ntrain.X <- as.matrix(Lag2[train])\ntest.X <- as.matrix(Lag2[!train])\ntrain.Direction <- Direction[train]\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Direction, k = 1)\ntable(knn.pred, Direction_test)\nmean(knn.pred == Direction_test)\n31/(31+30) #True positive rate/sensitivity\n21/(21 + 22) #True negative rate/specificity\n22/(21 + 22) #False positive rate\n\n# h\nmean(glm.pred == Direction_test)\nmean(lda.class == Direction_test)\nmean(qda.class == Direction_test)\nmean(knn.pred == Direction_test)\n\n\n#i\nplot(Volume)\nplot(exp(Volume))\nplot(log(Volume))\nplot(sqrt(Volume))\nglm.fit2 <- glm(\n  Direction ~ log(Volume),\n  data = Weekly,\n  family = binomial,\n  subset = train\n)\nsummary(glm.fit2)\nplot(Volume, Direction)\ndetach(Weekly)\n\n# 11 ====\nrm(list = ls())\nattach(Auto)\nhead(Auto)\nsummary(Auto)\n# a\nmpg01 <- as.integer(mpg > median(mpg))\nmpg01\nAuto$mpg01 <- mpg01\nsummary(Auto)\nmean(mpg01)\n\n# b\ncor(Auto[-9])\nplot(Auto)\nboxplot(cylinders ~ mpg01, main = \"Cylinders vs. mpg01\")\nboxplot(displacement ~ mpg01, main = \"Displacement vs. mpg01\")\nboxplot(horsepower ~ mpg01, main = \"Horespower vs. mpg01\")\nboxplot(weight ~ mpg01, main = \"Weight vs. mpg01\")\nplot(displacement, horsepower, pch = mpg01)\n?plot\n\n# c\ntrain <- year %% 2 == 0\ntable(train)\nAuto.train <- Auto[train,]\nAuto.test <- Auto[!train,]\nmpg01.test <- mpg01[!train]\n\n\n# d\nlda.fit <- lda(\n  mpg01 ~ cylinders + displacement + horsepower + weight,\n  data = Auto,\n  subset = train\n)\nlda.fit\nlda.pred <- predict(\n  lda.fit,\n  Auto.test\n)\nnames(lda.pred)\nlda.class <- lda.pred$class\ntable(lda.class, mpg01.test)\nmean(lda.class != mpg01.test) #Test error rate\n\n# e\nqda.fit <- qda(\n  mpg01 ~ cylinders + displacement + horsepower + weight,\n  data = Auto,\n  subset = train\n)\nqda.fit\nqda.pred <- predict(\n  qda.fit,\n  Auto.test\n)\nqda.class <- qda.pred$class\ntable(qda.class, mpg01.test)\nmean(qda.class != mpg01.test) #Test error rate\n\n# f\nglm.fit <- glm(\n  mpg01 ~ cylinders + displacement + horsepower + weight,\n  data = Auto,\n  family = binomial,\n  subset = train\n)\nglm.probs <- predict(\n  glm.fit,\n  newdata = Auto.test,\n  type = \"response\"\n)\nglm.pred <- rep(0, length(mpg01.test))\nglm.pred[glm.probs > 0.5] <- 1\ntable(glm.pred, mpg01.test)\nmean(glm.pred != mpg01.test) #Test error rate\n\n# g\ntrain.X <- cbind(cylinders, displacement, horsepower, weight)[train,]\ntest.X <- cbind(cylinders, displacement, horsepower, weight)[!train,]\ntrain.Direction <- mpg01[train]\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Direction, k = 1)\ntable(knn.pred, mpg01.test)\nmean(knn.pred != mpg01.test) #Test error rate\nfor(k in c(1, 3, 5, 10, 50, 100)){\n  set.seed(1)\n  knn.pred <- knn(train.X, test.X, train.Direction, k = k)\n  print(k)\n  print(mean(knn.pred != mpg01.test))\n}\n\n# 12====\n# a\nPower <- function(){\n  print(2^3)  \n}\nPower()\n\n# b\nPower2 <- function(x, a){\n  x^a\n}\nPower2(3,8)\n\n# c\nPower2(10, 3)\nPower2(8, 17)\nPower2(131, 3)\n\n# d\nPower3 <- function(x, a){\n  result <- x ^ a\n  return(result)\n}\n# e\nx <- 1:10\ny <- Power3(x, 2)\ny\nplot(x, y)\nplot(x, log(y))\nplot(x, y, log = \"xy\",\n     main = \"Y vs. X\",\n     xlab = \"log(x)\",\n     ylab = \"log(x ^ 2)\")\n\n# f\nPlotPower <- function(x, a) {\n  y <- x ^ a\n  plot(x, y, xlab = \"x\", ylab = \"x ^ a\")\n}\nPlotPower(1:100, 2)\ndetach(Auto)\nrm(list = ls())\n\n# 13====\nlibrary(MASS)\ndata(\"Boston\")\nattach(Boston)\nhead(Boston)\ncrim01 <- as.integer(crim > median(crim))\nBoston$crim01 <- crim01\n\ntrain <- 1:(length(crim01)/2)\nBoston.train <- Boston[train, ]\nBoston.test <- Boston[-train, ]\ncrim01.test <- crim01[-train]\npairs(Boston)\ncor(Boston)\n# Logistic regression\nglm.fit <- glm(\n  crim01 ~ . -crim -crim01 -chas -nox -tax -lstat,\n  data = Boston,\n  family = binomial,\n  subset = train\n)\nsummary(glm.fit)\n\nglm.probs <- predict(\n  glm.fit,\n  newdata = Boston.test,\n  type = \"response\"\n)\nglm.pred <- rep(0, length(crim01.test))\nglm.pred[glm.probs > 0.5] <- 1\ntable(glm.pred, crim01.test)\nmean(glm.pred != crim01.test) #Test error rate\n\n# LDA\nlda.fit <- lda(\n  crim01 ~ . -crim -crim01 -chas -nox -tax -lstat,\n  data = Boston,\n  subset = train\n)\nplot(lda.fit)\nlda.pred <- predict(\n  lda.fit,\n  Boston.test\n)\nlda.class <- lda.pred$class\ntable(lda.class, crim01.test)\nmean(lda.class != crim01.test) #Test error rate\n\n# QDA\nqda.fit <- qda(\n  crim01 ~ . -crim -crim01 -chas -nox -tax -lstat,\n  data = Boston,\n  subset = train\n)\nqda.pred <- predict(\n  qda.fit,\n  Boston.test\n)\nqda.class <- qda.pred$class\ntable(qda.class, crim01.test)\nmean(qda.class != crim01.test)\n\n# KNN\nlibrary(class)\nnames(Boston.train)\ntrain.X <- Boston.train[,-c(1, 15)]\ntest.X <- Boston.test[, -c(1, 15)]\ntrain.crim01 <- crim01[train]\n\nknn.pred <- knn(\n  train.X,\n  test.X,\n  train.crim01,\n  k = 1\n)\ntable(knn.pred, crim01.test)\nmean(knn.pred != crim01.test) #Test error rate\n\nknn.pred <- knn(\n  train.X,\n  test.X,\n  train.crim01,\n  k = 5\n)\ntable(knn.pred, crim01.test)\nmean(knn.pred != crim01.test) #Test error rate\n\nfor(k in c(1, 5, 10, 50, 100)) {\n  print(k)\n  knn.pred <- knn(\n    train.X,\n    test.X,\n    train.crim01,\n    k = k\n  )\n  \n  print(mean(knn.pred != crim01.test))\n}\n",
    "created" : 1484546699293.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2732213021",
    "id" : "CA6E09",
    "lastKnownWriteTime" : 1484740852,
    "last_content_update" : 1484740852872,
    "path" : "~/DataScience/ISLR/4_Classification.R",
    "project_path" : "4_Classification.R",
    "properties" : {
        "docOutlineVisible" : "1",
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}