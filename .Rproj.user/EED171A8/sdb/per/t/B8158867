{
    "collab_server" : "",
    "contents" : "library(ISLR)\nadvertising <- read.csv(\"Advertising.csv\", header = T, row.names = 1)\nhead(advertising)\nfit_tv <- lm(Sales ~ TV, data = advertising)\nsummary(fit_tv)\nfit_radio <- lm(Sales ~ Radio, data = advertising)\nfit_newpaper <- lm(Sales ~ Newspaper, data = advertising)\nsummary(fit_radio)\nsummary(fit_newpaper)\nfit <- lm(Sales ~ TV + Radio + Newspaper, data = advertising)\nsummary(fit)\ncor(advertising)\n\nfit_tv_radio <- lm(Sales ~ TV + Radio, data = advertising)\nsummary(fit_tv_radio)\n\ncredit <- read.csv(\"credit.csv\", header = T, row.names = 1)\nhead(credit)\npairs(credit)\ngender <- lm(Balance ~ Gender, data = credit)\nsummary(gender)\nsummary(credit$Ethnicity)\n\nethnicity <- lm(Balance ~ Ethnicity, data = credit)\nsummary(ethnicity)\n\nadvert_inter <- lm(Sales ~ TV + Radio + TV*Radio, \n                   data = advertising)\nsummary(advert_inter)\n\ncredit_income_student <- lm(Balance ~ Income + Student,\n                            data = credit)\nsummary(credit_income_student)\ncredit_income_student_inter <- lm(Balance ~ Income +\n                                    Student + \n                                    Income * Student, \n                                  data = credit)\nsummary(credit_income_student_inter)\n\ndata(\"Auto\")\nplot(Auto$horsepower, Auto$mpg)\nabline(lm(Auto$mpg ~ Auto$horsepower), col = 'red')\nhorsepower <- lm(mpg ~ horsepower, data = Auto)\nhorsepower_quad <- lm(mpg ~ horsepower + I(horsepower ^ 2),\n                      data = Auto)\nsummary(horsepower)\nsummary(horsepower_quad)\n\nauto_horsepower <- lm(mpg ~ horsepower, data = Auto)\nplot(auto_horsepower)\nplot(horsepower_quad)\nplot(horsepower_quad, which = 1)\n\ncredit_age_limit <- lm(Balance ~ Age + Limit, data = credit)\ncredit_rating_limit <- lm(\n  Balance ~ Rating + Limit, \n  data = credit\n)\nsummary(credit_age_limit)\nsummary(credit_rating_limit)\nlibrary(car) # http://www.statmethods.net/stats/rdiagnostics.html\nvif(credit_rating_limit)\nvif(credit_age_limit)\noutlierTest(credit_rating_limit)\nqqplot(credit_rating_limit)\nleveragePlots(credit_rating_limit)\navPlots(credit_rating_limit)\n\n# 3.4 The marketing plan\nhead(advertising)\nsummary(fit)\nmean(advertising$Sales)\n\n# 3.6 Lab: Linear Regression\nls()\nrm(list = ls())\nlibrary(MASS)\nlibrary(ISLR)\n\n# 3.6.2 Simple linear regression\nfix(Boston)\nnames(Boston)\nlm.fit <- lm(medv ~ lstat, data = Boston)\nattach(Boston)\nlm.fit <- lm(medv ~ lstat)\nlm.fit\nsummary(lm.fit)\nnames(lm.fit)\ncoef(lm.fit)\n# to obtain a confidence interval for the coefficient estimates:\nconfint(lm.fit)\n# The predict() function can be used to produce confidence intervals and\n# prediction intervals for the prediction of medv for a given value of lstat:\npredict(lm.fit, data.frame(lstat = c(5, 10, 15)),\n        interval = \"confidence\")\npredict(lm.fit, data.frame(lstat = c(5, 10, 15)),\n        interval = \"prediction\")\n# We will now plot medv and lstat along with the least squares regression\n# line using the plot() and abline() functions:\nplot(lstat, medv)\nabline(lm.fit)\nabline(lm.fit, lwd = 3)\nabline(lm.fit, lwd = 3, col = 'red')\nplot(lstat, medv, col = 'red')\nplot(lstat, medv, pch = 20)\nplot(lstat, medv, pch = \"+\")\nplot(1:20, 1:20, pch = 1:20)\n\npar(mfrow = c(2,2))\nplot(lm.fit)\n# Alternatively, we can compute the residuals from a linear regression fit\n# using the residuals() function. The function rstudent() will return the\n# studentized residuals, and we can use this function to plot the residuals\n# against the fitted values.\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\n# On the basis of the residual plots, there is some evidence of non-linearity.\n# Leverage statistics can be computed for any number of predictors using the\n# hatvalues() function.\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n# 3.6.3 Multiple linear regression\nlm.fit <- lm(medv ~ lstat + age)\nsummary(lm.fit)\nlm.fit <- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\nsummary(lm.fit)$sigma\nvif(lm.fit)\n# What if we would like to perform a regression using all of the variables but\n# one? For example, in the above regression output, age has a high p-value.\n# So we may wish to run a regression excluding this predictor. The following\n# syntax results in a regression using all predictors except age:\nlm.fit1 <- lm(medv ~ . -age, data = Boston)\nsummary(lm.fit1)\n# Alternatively, the update() function can be used.\nlm.fit1 <- update(lm.fit, ~ . -age)\n\n# 3.6.4 Interaction terms\n# It is easy to include interaction terms in a linear model using the lm() function.\n# The syntax lstat:black tells R to include an interaction term between\n# lstat and black. The syntax lstat*age simultaneously includes lstat, age,\n# and the interaction term lstat×age as predictors; it is a shorthand for\n# lstat+age+lstat:age.\nsummary(lm(medv ~ lstat * age))\n\n# 3.6.5 Non-linear tranformations of the predictors\n# The lm() function can also accommodate non-linear transformations of the\n# predictors. For instance, given a predictor X, we can create a predictor X2\n# using I(X^2). The function I() is needed since the ^ has a special meaning\n# I()\n# in a formula; wrapping as we do allows the standard usage in R, which is\n# to raise X to the power 2. We now perform a regression of medv onto lstat\n# and lstat2\nlm.fit2 <- lm(medv ~ lstat + I(lstat ^ 2))\nsummary(lm.fit2)\nlm.fit <- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\nplot()\nplot(1:20, 1:20, pch = 1:20)\nplot(lm.fit2)\n\n# The lm() function can also accommodate non-linear transformations of the\n# predictors. For instance, given a predictor X, we can create a predictor X2\n# using I(X^2). The function I() is needed since the ^ has a special meaning\n# I()\n# in a formula; wrapping as we do allows the standard usage in R, which is\n# to raise X to the power 2. We now perform a regression of medv onto lstat\n# and lstat2\nlm.fit5 <- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\nsummary(lm(medv ~ log(rm)))\ndetach(Boston)\n\n# 3.6.6 Qualitative predictors\nfix(Carseats)\nnames(Carseats)\nattach(Carseats)\nlm.fit <- lm(Sales ~ . +Income:Advertising + Price:Age,\n             data = Carseats)\nsummary(lm.fit)\n# The contrasts() function returns the coding that R uses for the dummy\n# variables.\ncontrasts(ShelveLoc)\n?contrasts\ndetach(Carseats)\n\n# 3.7 Exercises\nGPA <- 4\nIQ <- 110\n85 + 10 * GPA + 0.07 * IQ + 0.01 * (GPA * IQ)\n\n# 8\nfit_8 <- lm(mpg ~ horsepower, data = Auto)\nsummary(fit_8)\nmean(Auto$mpg)\nnames(fit_8)\nsummary(fit_8)$sigma\nsummary(fit_8)$sigma/mean(Auto$mpg)\npredict(fit_8, data.frame(horsepower = 98))\npredict(fit_8, data.frame(horsepower = 98),\n        interval = \"confidence\")\npredict(fit_8, data.frame(horsepower = 98),\n        interval = \"prediction\")\nplot(Auto$horsepower, Auto$mpg, \n     xlab = \"horsepower\",\n     ylab = \"mpg\",\n     main = \"Auto data\") \nabline(fit_8, col = 'red')\nplot(fit_8)\npar(mfrow = c(1,1))\n\n# 9\npairs(Auto)\nnames(Auto)\ncor(Auto[1:8])\nlm_9 <- lm(mpg ~ . -name, data = Auto)\nsummary(lm_9)\n# p-value on F-statistic indicates relationship\n# all but cylinders, horsepower and acceleration are sig.\n# coef of year indicates positive relationship...increasse in 1 year\n# on averague has the effect of an increase of 0.75 mpg\npar(mfrow = c(2,2))\nplot(lm_9)\n# non-linearity, some outlier, some high-leverarge points\nlm_9.2 <- lm(mpg ~ cylinders * displacement +\n               displacement * weight, data = Auto)\nsummary(lm_9.2)\n# Interaction between displacement and weight is sig., between cyl and\n# displacement is not\nplot(Auto$displacement, Auto$mpg)\nplot(log(Auto$displacement), Auto$mpg)\n\n# 10\n# a\nlm_10 <- lm(Sales ~ Price + Urban + US,\n            data = Carseats)\nsummary(lm_10)\n# e\nlm_10a <- lm(Sales ~ Price + US,\n             data = Carseats)\nsummary(lm_10a)\n# g\nconfint(lm_10a)\npar()\nplot(lm_10a)\nplot()\nplot(predict(lm_10a), rstudent(lm_10a))\nplot(hatvalues(lm_10a))\nplot(rstudent(lm_10a), hatvalues(lm_10a))\nplot(hatvalues(lm_10a), rstudent(lm_10a))\n\n# 11\nset.seed(1)\nx <- rnorm(100)\ny <- 2 * x + rnorm(100)\n# a\nlm_11 <- lm(y ~ x + 0)\nsummary(lm_11)\n# coef = 1.99, se = 0.1065, t statistic = 18.73, p-value = 0\n# b\nlm_11b <- lm(x ~ y + 0)\nsummary(lm_11b)\n# coef B = 0.3911, se = .0209, t statistic = 18.73, p=value = 0 \n# c \n# We obtain the\n# same value for the t-statistic and consequently the same value for the\n# corresponding p-value. Both results in (a) and (b) reflect the same line\n# created in (a). In other words, y=2x+εy=2x+ε could also be written\n# x=0.5(y−ε)x=0.5(y−ε).\n\n# d\n# e\n# f\nlm_11f1 <- lm(y ~ x)\nlm_11f2 <- lm(x ~ y)\nsummary(lm_11f1)\nsummary(lm_11f2)\n\n# 12\n# 13\nset.seed(1)\nx <- rnorm(100)\neps <- rnorm(100, 0, sd = sqrt(0.25))\ny <- -1 + 0.5 * x + eps\nlength(y)\n# B0 = -1, B1 = 0.5\nplot(x, y)\nlm_13 <- lm(y ~ x)\nsummary(lm_13)\nplot(x, y)\nabline(lm_13, col = 'red')\nabline(-1, 0.5, col = 'blue')\nlegend(\"topleft\", c(\"Least square\", \"Regression\"), col = c(\"red\", \"blue\"), lty = c(1,1))\n??legend.lty\n?plot\n?legend\nlm_13q <- lm(y ~ x + I(x^2))\nsummary(lm_13q)\n# h\nset.seed(1)\neps <- rnorm(100, sd = 0.125)\nx <- rnorm(100)\ny <- -1 + 0.5 * x + eps\nplot(x, y)\nlm_13a <- lm(y ~ x)\nsummary(lm_13a)\n# i\nset.seed(1)\neps <- rnorm(100, sd = 1)\nx <- rnorm(100)\ny <-  -1 + 0.5 * x + eps\nlm_13more <- lm(y ~ x)\nsummary(lm_13more)\nplot(x, y)\nabline(lm_13more, col = 'red')\nabline(-1, 0.5, col = \"blue\")\n\n# j\nconfint(lm_13)\nconfint(lm_13a)\nconfint(lm_13more)\n\n# 14\nset.seed(1)\nx1 = runif(100)\nx2 = 0.5 * x1 + rnorm(100)/10\ny = 2 + 2 * x1 + 0.3 * x2 + rnorm(100)\n# a B0 = 2, B1 = 2, B3 = 0.3\n# b\ncor(x1, x2)\nplot(x1, x2)\nfit_14 <- lm(y ~ x1 + x2)\nsummary(fit_14)\n# d\nfit_14d <- lm(y ~ x1)\nsummary(fit_14d)\n# e\nfit_14e <- lm(y ~ x2)\nsummary(fit_14e)\n# g\nx1 <- c(x1, 0.1)\nx2 <- c(x2, 0.8)\ny <- c(y, 6)\nfit_14g1 <- lm(y ~ x1 + x2)\nfit_14g2 <- lm(y ~ x1)\nfit_14g3 <- lm(y ~ x2)\nsummary(fit_14g1)\nsummary(fit_14g2)\nsummary(fit_14g3)\nplot(1:100, 100:1)\nplot(fit_14g1)\nplot(fit_14g2)\nplot(fit_14g3)\n\n# 15\n# a\nnames(Boston)\n?Boston\nfit_zn <- lm(crim ~ zn, data = Boston)\nsummary(fit_zn)\nfit_indus <- lm(crim ~ indus, data = Boston)\nsummary(fit_indus)\nfit_chas <- lm(crim ~ chas, data = Boston)\nsummary(fit_chas)\nfit_nox <- lm(crim ~ nox, data = Boston)\nsummary(fit_nox)\nfit_rm <- lm(crim ~ rm, data = Boston)\nsummary(fit_rm)\nfit_age <- lm(crim ~ age, data = Boston)\nsummary(fit_age)\nfit_dis <- lm(crim ~ age, data = Boston)\nsummary(fit_dis)\nfit_rad <- lm(crim ~ rad, data = Boston)\nsummary(fit_rad)\nfit_tax <- lm(crim ~ tax, data = Boston)\nsummary(fit_tax)\nfit_ptratio <- lm(crim ~ ptratio, data = Boston)\nsummary(fit_ptratio)\nfit_black <- lm(crim ~ black, data = Boston)\nsummary(fit_black)\nfit_lstat <- lm(crim ~ lstat, data = Boston)\nsummary(fit_lstat)\nfit_medv <- lm(crim ~ medv, data = Boston)\nsummary(fit_medv)\n# To find which predictors are significant, we have to test H0:β1=0H0:β1=0. All\n# predictors have a p-value less than 0.05 except “chas”, so we may conclude\n# that there is a statistically significant association between each predictor\n# and the response except for the “chas” predictor.\n\n# b\nfit_all <- lm(crim ~ ., data = Boston)\nsummary(fit_all)\n# reject null for zn, dis, rad, black, medv\n\n# c\nmult_reg <- numeric()\nfit_all$coefficients\nmult_reg <- c(mult_reg, fit_all$coefficients[-1])\nmult_reg\nattributes(mult_reg)\n\nsimple_reg <- numeric()\nsimple_reg <- c(simple_reg, fit_zn$coefficients[2])\nsimple_reg <- c(simple_reg, fit_indus$coefficients[2])\nsimple_reg <- c(simple_reg, fit_chas$coefficients[2])\nsimple_reg <- c(simple_reg, fit_nox$coefficients[2])\nsimple_reg <- c(simple_reg, fit_rm$coefficients[2])\nsimple_reg <- c(simple_reg, fit_age$coefficients[2])\nsimple_reg <- c(simple_reg, fit_dis$coefficients[2])\nsimple_reg <- c(simple_reg, fit_rad$coefficients[2])\nsimple_reg <- c(simple_reg, fit_tax$coefficients[2])\nsimple_reg <- c(simple_reg, fit_ptratio$coefficients[2])\nsimple_reg <- c(simple_reg, fit_black$coefficients[2])\nsimple_reg <- c(simple_reg, fit_lstat$coefficients[2])\nsimple_reg <- c(simple_reg, fit_medv$coefficients[2])\nlength(simple_reg)\nlength(mult_reg)\npar(mfrow = c(1, 1))\nplot(simple_reg, mult_reg)\nplot(simple_reg[-4], mult_reg[-4])\nsimple_reg\nmult_reg\n\n# There is a difference between the simple and multiple regression coefficients.\n# This difference is due to the fact that in the simple regression case, the\n# slope term represents the average effect of an increase in the predictor,\n# ignoring other predictors. In contrast, in the multiple regression case, the\n# slope term represents the average effect of an increase in the predictor,\n# while holding other predictors fixed. It does make sense for the multiple\n# regression to suggest no relationship between the response and some of the\n# predictors while the simple linear regression implies the opposite because the\n# correlation between the predictors show some strong relationships between some\n# of the predictors.\ncor(Boston[-c(1, 4)])\n# So for example, when “age” is high there is a tendency in “dis” to be low,\n# hence in simple linear regression which only examines “crim” versus “age”, we\n# observe that higher values of “age” are associated with higher values of\n# “crim”, even though “age” does not actually affect “crim”. So “age” is a\n# surrogate for “dis”; “age” gets credit for the effect of “dis” on “crim”.\n\n# d\nfit_zn2 <- lm(crim ~ poly(zn, 3), data = Boston)\nsummary(fit_zn2)\nsummary(fit_zn)\nfit_indus2 <- lm(crim ~ poly(indus, 3), data = Boston)\nsummary(fit_indus2)\nfit_nox2 <- lm(crim ~ poly(nox, 3), data = Boston)\nsummary(fit_nox2)\nfit_rm2 <- lm(crim ~ poly(rm, 3), data = Boston)\nsummary(fit_rm2)\nfit_age2 <- lm(crim ~ poly(age, 3), data = Boston)\nsummary(fit_age2)\nfit_dis2 <- lm(crim ~ poly(dis, 3), data = Boston)\nsummary(fit_dis2)\nfit_rad2 <- lm(crim ~ poly(rad, 3), data = Boston)\nsummary(fit_rad2)\nfit_tax2 <- lm(crim ~ poly(tax, 3), data = Boston)\nsummary(fit_tax2)\nfit_ptratio2 <- lm(crim ~ poly(ptratio, 3), data = Boston)\nsummary(fit_ptratio2)\nfit_black2 <- lm(crim ~ poly(black, 3), data = Boston)\nsummary(fit_black2)\nfit_lstat2 <- lm(crim ~ poly(lstat, 3), data = Boston)\nsummary(fit_lstat2)\nfit_medv2 <- lm(crim ~ poly(medv, 3), data = Boston)\nsummary(fit_medv2)\n\n# For “zn”, “rm”, “rad”, “tax” and “lstat” as predictor, the p-values suggest\n# that the cubic coefficient is not statistically significant; for “indus”,\n# “nox”, “age”, “dis”, “ptratio” and “medv” as predictor, the p-values suggest\n# the adequacy of the cubic fit; for “black” as predictor, the p-values suggest\n# that the quandratic and cubic coefficients are not statistically significant,\n# so in this latter case no non-linear effect is visible.\n\nmedv2_line <- predict(fit_medv2, data.frame(medv = 5:50))\nplot(Boston$medv, Boston$crim)\nabline(fit_medv, col = 'blue')\nlines(medv2_line, col = 'red')\nplot(Boston$indus, Boston$crim)\n\nnox2_line <- predict(fit_nox2, data.frame(nox = 0.4:0.9))\nplot(Boston$nox, Boston$crim)\nlines(nox2_line, col = 'red')\n",
    "created" : 1482523108927.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3846451827",
    "id" : "B8158867",
    "lastKnownWriteTime" : 1483190456,
    "last_content_update" : 1483190456237,
    "path" : "~/DataScience/ISLR/3_Linear_Regression.R",
    "project_path" : "3_Linear_Regression.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}